import os
import base64
from datetime import datetime

import pandas as pd
import numpy as np

import matplotlib
matplotlib.use("Agg")  
import matplotlib.pyplot as plt

import seaborn as sns
import plotly.express as px

from sklearn.cluster import KMeans
from sklearn.preprocessing import RobustScaler, MinMaxScaler
from streamlit_option_menu import option_menu
from scipy.stats.mstats import winsorize
from sklearn.metrics import silhouette_score

import pickle
import streamlit as st
import squarify

# H√†m ti·ªán √≠ch
def check_columns(df, required_cols, context=""):
    """Ki·ªÉm tra c√°c c·ªôt b·∫Øt bu·ªôc c√≥ trong DataFrame kh√¥ng"""
    missing = [col for col in required_cols if col not in df.columns]
    if missing:
        st.error(f"‚ùå Thi·∫øu c·ªôt {missing} trong d·ªØ li·ªáu {context}. Vui l√≤ng ki·ªÉm tra l·∫°i file input.")
        return False
    return True

# GUI setup
st.set_page_config(layout="wide")
st.title("Data Science & Machine Learning Project")
st.header("Customer Segmentation", divider='rainbow')

# Menu
with st.sidebar:
    choice = option_menu(
        "üìä Menu ch√≠nh",
        ["Business Understanding", "Data Understanding", "Data Preparation", "Modeling & Evaluation", "Predict"],
        icons=["briefcase", "database", "gear", "bar-chart", "magic"],
        menu_icon="cast",
        default_index=0
    )

def load_data(uploaded_file, default_path=None):
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file, encoding='latin-1')
        st.sidebar.success("File uploaded successfully!")
    elif default_path is not None and os.path.exists(default_path):
        try:
            df = pd.read_csv(default_path, encoding='latin-1')
            st.sidebar.info(f"D√πng file m·∫´u: {os.path.basename(default_path)}")
        except Exception as e:
            st.error(f"‚ùå Kh√¥ng th·ªÉ ƒë·ªçc file m·∫´u {default_path}: {e}")
            df = None
    else:
        df = None
    return df

def csv_download_link(df, csv_file_name, download_link_text):
    csv_data = df.to_csv(index=False)
    b64 = base64.b64encode(csv_data.encode()).decode()
    href = f'<a href="data:file/csv;base64,{b64}" download="{csv_file_name}">{download_link_text}</a>'
    st.markdown(href, unsafe_allow_html=True)

# Initializing session state variables
if 'df' not in st.session_state:
    st.session_state['df'] = None

# Business Understanding

if choice == 'Business Understanding':
    st.subheader("Business Objective")
    st.write("""
    ###### Ph√¢n kh√∫c/nh√≥m/c·ª•m kh√°ch h√†ng (market segmentation ‚Äì c√≤n ƒë∆∞·ª£c g·ªçi l√† ph√¢n kh√∫c th·ªã tr∆∞·ªùng) l√† qu√° tr√¨nh nh√≥m c√°c kh√°ch h√†ng l·∫°i v·ªõi nhau d·ª±a tr√™n c√°c ƒë·∫∑c ƒëi·ªÉm chung. N√≥ ph√¢n chia v√† nh√≥m kh√°ch h√†ng th√†nh c√°c nh√≥m nh·ªè theo ƒë·∫∑c ƒëi·ªÉm ƒë·ªãa l√Ω, nh√¢n kh·∫©u h·ªçc, t√¢m l√Ω h·ªçc, h√†nh vi (geographic, demographic, psychographic, behavioral) v√† c√°c ƒë·∫∑c ƒëi·ªÉm kh√°c.

    ###### C√°c nh√† ti·∫øp th·ªã s·ª≠ d·ª•ng k·ªπ thu·∫≠t n√†y ƒë·ªÉ:
    - **C√° nh√¢n h√≥a**: ƒêi·ªÅu ch·ªânh chi·∫øn l∆∞·ª£c ti·∫øp th·ªã ƒë·ªÉ ƒë√°p ·ª©ng nhu c·∫ßu ri√™ng c·ªßa t·ª´ng ph√¢n kh√∫c.
    - **T·ªëi ∆∞u h√≥a**: Ph√¢n b·ªï hi·ªáu qu·∫£ ngu·ªìn l·ª±c ti·∫øp th·ªã.
    - **Insight**: Hi·ªÉu s√¢u h∆°n v·ªÅ c∆° s·ªü kh√°ch h√†ng.
    - **T∆∞∆°ng t√°c**: N√¢ng cao s·ª± t∆∞∆°ng t√°c v√† s·ª± h√†i l√≤ng c·ªßa kh√°ch h√†ng.

    ###### => V·∫•n ƒë·ªÅ/Y√™u c·∫ßu: S·ª≠ d·ª•ng Machine Learning v√† ph√¢n t√≠ch d·ªØ li·ªáu trong Python ƒë·ªÉ th·ª±c hi·ªán ph√¢n kh√∫c kh√°ch h√†ng.
    """)
    st.image("RFM.png", caption="Customer Segmentation s·ª≠ d·ª•ng RFM", use_container_width=True)

# Data Understanding

elif choice == 'Data Understanding':
    st.subheader("Upload ho·∫∑c ch·ªçn d·ªØ li·ªáu")

    # Upload d·ªØ li·ªáu
    st.markdown("**üì¶ Product File (th√¥ng tin s·∫£n ph·∫©m)**")
    product_file = st.sidebar.file_uploader("Upload product file", type=['csv'], key="product")
    df_products = load_data(product_file, default_path="Data Project 1/Products_with_Categories.csv")

    st.markdown("**üõí Transaction File (giao d·ªãch)**")
    transaction_file = st.sidebar.file_uploader("Upload transaction file", type=['csv'], key="transaction")
    df_transactions = load_data(transaction_file, default_path="Data Project 1/Transactions.csv")

    if df_products is not None and df_transactions is not None:
        if "productId" not in df_transactions.columns:
            st.error("Kh√¥ng t√¨m th·∫•y c·ªôt 'productId' trong transaction file.")
        else:
            df = pd.merge(df_transactions, df_products, on="productId", how="left")
            st.session_state['df'] = df

            # T·∫°o transactionId = Member_number + Date (coi m·ªói KH trong 1 ng√†y l√† 1 giao d·ªãch)
            df["transactionId"] = df["Member_number"].astype(str) + "_" + df["Date"].astype(str)

            st.write("### Data Overview")
            st.write("S·ªë d√≤ng:", df.shape[0])
            st.write("S·ªë c·ªôt:", df.shape[1])

            # Tabs trong Data Understanding

            tab1, tab2, tab3, tab4, tab5 = st.tabs(["‚è∞ Th·ªùi gian", "üìÇ Lo·∫°i H√†ng ho√°", "üõçÔ∏è S·∫£n ph·∫©m", "üõçÔ∏è S·∫£n ph·∫©m ƒëi k√®m", "üë§ Kh√°ch h√†ng"])

            with tab1:  # Th·ªùi gian
                st.markdown("### ‚è∞ Ph√¢n t√≠ch Doanh thu theo Th·ªùi gian")

                # Chuy·ªÉn c·ªôt Date sang datetime
                df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')

                # Th√™m c·ªôt Month v√† Year
                df['Month'] = df['Date'].dt.to_period("M").astype(str)
                df['Year'] = df['Date'].dt.year

                # B·ªô ch·ªçn m·ª©c ph√¢n t√≠ch
                view_option = st.selectbox(
                    "Ch·ªçn m·ª©c ph√¢n t√≠ch th·ªùi gian:",
                    ["Ng√†y", "Th√°ng", "NƒÉm"]
                )

                if view_option == "Ng√†y":
                    daily_sales = df.groupby('Date')['price'].sum().reset_index()
                    st.markdown("#### üìÖ Doanh thu theo Ng√†y")
                    st.line_chart(daily_sales.set_index("Date"))

                elif view_option == "Th√°ng":
                    monthly_sales = df.groupby('Month')['price'].sum().reset_index()
                    st.markdown("#### üìÜ Doanh thu theo Th√°ng")
                    st.line_chart(monthly_sales.set_index("Month"))

                elif view_option == "NƒÉm":
                    yearly_sales = df.groupby('Year')['price'].sum().reset_index()
                    st.markdown("#### üìä Doanh thu theo NƒÉm")
                    st.bar_chart(yearly_sales.set_index("Year"))


            with tab2:  # Lo·∫°i h√†ng ho√°
                st.markdown("### üìÇ Ph√¢n t√≠ch theo Lo·∫°i h√†ng ho√°")

                if "Category" not in df.columns:
                    st.error("‚ùå Kh√¥ng t√¨m th·∫•y c·ªôt 'Category' trong d·ªØ li·ªáu. Vui l√≤ng ki·ªÉm tra l·∫°i file input.")
                else:
                    # 1Ô∏è‚É£ Doanh thu theo Category
                    revenue_by_cat = (
                        df.groupby("Category")["price"]
                        .sum()
                        .sort_values(ascending=False)
                        .head(10)
                        .reset_index()
                    )
                    st.markdown("#### üí∞ Top 10 Category theo Doanh thu")
                    st.bar_chart(revenue_by_cat.set_index("Category"))
                    st.dataframe(revenue_by_cat, use_container_width=True)

                    # 2Ô∏è‚É£ S·ªë ƒë∆°n h√†ng theo Category
                    orders_by_cat = (
                        df.groupby("Category")["productId"]
                        .count()
                        .sort_values(ascending=False)
                        .head(10)
                        .reset_index()
                    )
                    orders_by_cat.rename(columns={"productId": "Orders"}, inplace=True)
                    st.markdown("#### üì¶ Top 10 Category theo S·ªë ƒë∆°n h√†ng")
                    st.bar_chart(orders_by_cat.set_index("Category"))
                    st.dataframe(orders_by_cat, use_container_width=True)

                    # 3Ô∏è‚É£ S·ªë kh√°ch mua theo Category
                    customers_by_cat = (
                        df.groupby("Category")["Member_number"]
                        .nunique()
                        .sort_values(ascending=False)
                        .head(10)
                        .reset_index()
                    )
                    customers_by_cat.rename(columns={"Member_number": "Unique_Customers"}, inplace=True)
                    st.markdown("#### üë§ Top 10 Category theo S·ªë kh√°ch h√†ng mua")
                    st.bar_chart(customers_by_cat.set_index("Category"))
                    st.dataframe(customers_by_cat, use_container_width=True)

            with tab3:  # S·∫£n ph·∫©m
                st.markdown("### üõí Ph√¢n t√≠ch theo S·∫£n ph·∫©m")

                if "productName" not in df.columns:
                    st.error("‚ùå Kh√¥ng t√¨m th·∫•y c·ªôt 'productName' trong d·ªØ li·ªáu. Vui l√≤ng ki·ªÉm tra l·∫°i file input.")
                else:
                    # 1Ô∏è‚É£ Doanh thu theo s·∫£n ph·∫©m
                    revenue_by_prod = (
                        df.groupby("productName")["price"]
                        .sum()
                        .sort_values(ascending=False)
                        .head(10)
                        .reset_index()
                    )
                    st.markdown("#### üí∞ Top 10 S·∫£n ph·∫©m theo Doanh thu")
                    st.bar_chart(revenue_by_prod.set_index("productName"))
                    st.dataframe(revenue_by_prod, use_container_width=True)

                    # 2Ô∏è‚É£ S·ªë ƒë∆°n h√†ng theo s·∫£n ph·∫©m
                    orders_by_prod = (
                        df.groupby("productName")["productId"]
                        .count()
                        .sort_values(ascending=False)
                        .head(10)
                        .reset_index()
                    )
                    orders_by_prod.rename(columns={"productId": "Orders"}, inplace=True)
                    st.markdown("#### üì¶ Top 10 S·∫£n ph·∫©m theo S·ªë ƒë∆°n h√†ng")
                    st.bar_chart(orders_by_prod.set_index("productName"))
                    st.dataframe(orders_by_prod, use_container_width=True)

                    # 3Ô∏è‚É£ S·ªë kh√°ch mua theo s·∫£n ph·∫©m
                    customers_by_prod = (
                        df.groupby("productName")["Member_number"]
                        .nunique()
                        .sort_values(ascending=False)
                        .head(10)
                        .reset_index()
                    )
                    customers_by_prod.rename(columns={"Member_number": "Unique_Customers"}, inplace=True)
                    st.markdown("#### üë§ Top 10 S·∫£n ph·∫©m theo S·ªë kh√°ch h√†ng mua")
                    st.bar_chart(customers_by_prod.set_index("productName"))
                    st.dataframe(customers_by_prod, use_container_width=True)

            with tab4: # TAB: S·∫£n ph·∫©m ƒëi k√®m
                st.subheader("üîó Ph√¢n t√≠ch s·∫£n ph·∫©m th∆∞·ªùng ƒë∆∞·ª£c mua k√®m")

                # ƒê·∫£m b·∫£o c√≥ transactionId
                df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')
                df['transactionId'] = df['Member_number'].astype(str) + "_" + df['Date'].dt.strftime('%Y-%m-%d')

                # Pivot: productName x user
                pivot = df.groupby(['productName','Member_number'])['items'].sum().unstack(fill_value=0)
                pivot_binary = (pivot > 0).astype(int)
                items = pivot_binary.index.tolist()
                item_to_idx = {it:i for i,it in enumerate(items)}

                # Cosine similarity (item-based CF)
                from sklearn.metrics.pairwise import cosine_similarity
                item_sim = cosine_similarity(pivot_binary.values)

                # Shrinkage theo s·ªë user chung
                co_matrix = pivot_binary @ pivot_binary.T  # co-occurrence counts (item x item)
                shrinkage_alpha = 5
                n_users = pivot_binary.shape[1]
                sim_shrinked = (co_matrix.values / n_users) / (
                    (co_matrix.values + shrinkage_alpha) / (n_users + shrinkage_alpha)
                )
                item_sim = item_sim * sim_shrinked  # combine raw sim + shrinkage

                # Co-occurrence counts cho fallback
                from collections import Counter
                baskets = df.groupby('transactionId')['productName'].apply(lambda x: set(x)).tolist()
                co_counts = {}
                for basket in baskets:
                    for a in basket:
                        co_counts.setdefault(a, Counter())
                        for b in basket:
                            if a == b: 
                                continue
                            co_counts[a][b] += 1

                # Apriori rules
                from mlxtend.frequent_patterns import apriori, association_rules
                from mlxtend.preprocessing import TransactionEncoder

                transactions = df.groupby('transactionId')['productName'].apply(list).tolist()
                te = TransactionEncoder()
                te_ary = te.fit(transactions).transform(transactions)
                basket_df = pd.DataFrame(te_ary, columns=te.columns_)

                freq_items = apriori(basket_df, min_support=0.01, use_colnames=True)
                rules = association_rules(freq_items, metric="lift", min_threshold=1.0)
                rules = rules[(rules['lift'] > 1.1) & (rules['confidence'] > 0.15)]

                # Function l·∫•y ƒë·ªÅ xu·∫•t
                def get_combined_recs(prod, topn=10):
                    recs = []

                    # 1. T·ª´ CF
                    if prod in item_to_idx:
                        sims = item_sim[item_to_idx[prod]]
                        idxs = sims.argsort()[::-1]
                        for j in idxs:
                            if items[j] == prod: 
                                continue
                            recs.append((items[j], float(sims[j]), 'CF'))

                    # 2. T·ª´ Apriori
                    if not rules.empty:
                        related = rules[rules['antecedents'].apply(lambda x: prod in list(x))]
                        for _, row in related.iterrows():
                            for c in row['consequents']:
                                recs.append((c, float(row['confidence']), 'Apriori'))

                    # 3. T·ª´ Co-occurrence fallback
                    co = co_counts.get(prod, Counter())
                    for p, cnt in co.most_common(50):
                        recs.append((p, float(cnt), 'Co-occurrence'))

                    # N·∫øu ho√†n to√†n r·ªóng -> return []
                    if not recs:
                        return pd.DataFrame(columns=['product','score','source'])

                    # G·ªôp k·∫øt qu·∫£
                    dfc = pd.DataFrame(recs, columns=['product','score','source'])
                    dfc = dfc.groupby('product').agg({'score':'max','source':'first'}).reset_index()
                    dfc['score_norm'] = (dfc['score'] - dfc['score'].min()) / (dfc['score'].max() - dfc['score'].min() + 1e-9)
                    dfc = dfc.sort_values('score_norm', ascending=False).head(topn)
                    return dfc

                # UI
                selected = st.selectbox("Ch·ªçn s·∫£n ph·∫©m ƒë·ªÉ ph√¢n t√≠ch", sorted(df['productName'].unique()))

                if st.button("Xem ƒë·ªÅ xu·∫•t mua k√®m"):
                    rec_df = get_combined_recs(selected, topn=10)

                    if rec_df.empty:
                        st.warning("‚ùå Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m mua k√®m. H√£y th·ª≠ s·∫£n ph·∫©m kh√°c.")
                    else:
                        st.write("üìä ƒê·ªÅ xu·∫•t s·∫£n ph·∫©m th∆∞·ªùng mua k√®m v·ªõi:", selected)
                        st.dataframe(rec_df[['product','score_norm','source']])
                        st.bar_chart(rec_df.set_index('product')['score_norm'])

            with tab5:  # Kh√°ch h√†ng
                st.markdown("### üë• Ph√¢n t√≠ch theo Kh√°ch h√†ng")

                if "Member_number" not in df.columns or "price" not in df.columns:
                    st.error("‚ùå Thi·∫øu c·ªôt 'Member_number' ho·∫∑c 'price' trong d·ªØ li·ªáu.")
                else:
                    # Chu·∫©n h√≥a d·ªØ li·ªáu ng√†y
                    df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')

                    # T·ªïng s·ªë l·∫ßn mua c·ªßa kh√°ch h√†ng
                    purchase_count = (
                        df.groupby("Member_number")["productId"]
                        .count()
                        .sort_values(ascending=False)
                        .head(10)
                        .reset_index()
                    )
                    purchase_count.rename(columns={"productId": "Total_Purchases"}, inplace=True)
                    st.markdown("#### üõí Top 10 kh√°ch h√†ng theo T·ªïng s·ªë l·∫ßn mua")
                    st.bar_chart(purchase_count.set_index("Member_number"))
                    st.dataframe(purchase_count, use_container_width=True)

                    # T·ªïng chi ti√™u c·ªßa kh√°ch h√†ng
                    total_spent = (
                        df.groupby("Member_number")["price"]
                        .sum()
                        .sort_values(ascending=False)
                        .head(10)
                        .reset_index()
                    )
                    total_spent.rename(columns={"price": "Total_Spent"}, inplace=True)
                    st.markdown("#### üí∞ Top 10 kh√°ch h√†ng theo T·ªïng chi ti√™u")
                    st.bar_chart(total_spent.set_index("Member_number"))
                    st.dataframe(total_spent, use_container_width=True)

                    # Trung b√¨nh chi ti√™u m·ªói ƒë∆°n
                    avg_spent = (
                        df.groupby("Member_number")["price"]
                        .mean()
                        .sort_values(ascending=False)
                        .head(10)
                        .reset_index()
                    )
                    avg_spent.rename(columns={"price": "Avg_Spent_per_Order"}, inplace=True)
                    st.markdown("#### üìä Top 10 kh√°ch h√†ng theo Trung b√¨nh chi ti√™u m·ªói ƒë∆°n")
                    st.bar_chart(avg_spent.set_index("Member_number"))
                    st.dataframe(avg_spent, use_container_width=True)

                    # Ng√†y mua g·∫ßn nh·∫•t c·ªßa kh√°ch h√†ng
                    last_purchase = (
                        df.groupby("Member_number")["Date"]
                        .max()
                        .reset_index()
                    )
                    last_purchase = last_purchase.sort_values("Date", ascending=False).head(10)
                    last_purchase.rename(columns={"Date": "Last_Purchase_Date"}, inplace=True)
                    st.markdown("#### üìÖ Top 10 kh√°ch h√†ng c√≥ ng√†y mua g·∫ßn nh·∫•t")
                    st.dataframe(last_purchase, use_container_width=True)

            # Cho ph√©p t·∫£i CSV ƒë√£ merge
            csv_download_link(df, "merged_data.csv", "üì• Download merged CSV")
    else:
        st.warning("‚ö†Ô∏è Vui l√≤ng upload ho·∫∑c ki·ªÉm tra file m·∫´u trong th∆∞ m·ª•c `Data Project 1`")

if choice == 'Data Preparation':
    st.subheader("Data Preparation & EDA")

    if st.session_state['df'] is not None:
        df = st.session_state['df'].copy()

        if not check_columns(df, ["Date", "items", "price", "Member_number"], context="Transactions"):
            st.stop()
        # ƒê·∫£m b·∫£o Date v·ªÅ d·∫°ng datetime
        df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')
        df['Day'] = df['Date'].dt.to_period("D").dt.to_timestamp()

        # T·∫°o bi·∫øn Sales
        df['Sales'] = df['items'] * df['price']

        st.markdown("### üìä Data Summary")
        st.write(df.describe())

        # Missing values
        st.markdown("### üï≥ Missing Values")
        st.write(df.isna().sum())

        # Duplicate check
        dup_cnt = df.duplicated(subset=["Member_number", "Date", "price"]).sum()
        st.write(f"**Likely duplicates**: {dup_cnt} rows (subset on Member_number, Date, price)")

        # Histograms
        st.markdown("### Distribution of Price & log(1+Price)")
        fig, ax = plt.subplots(1, 2, figsize=(12, 4))
        ax[0].hist(df["price"], bins=40, alpha=.8)
        ax[0].set_title("Histogram price")
        ax[1].hist(np.log1p(df["price"]), bins=40, alpha=.8)
        ax[1].set_title("Histogram log(1+price)")
        st.pyplot(fig)

        # Daily revenue
        st.markdown("### üìà Daily Revenue with MA7")
        daily = df.groupby("Day")["price"].sum().sort_index()
        fig, ax = plt.subplots(figsize=(10, 4))
        ax.plot(daily.index, daily.values, alpha=.4, label="daily")
        ax.plot(daily.index, daily.rolling(7, min_periods=1).mean(), label="7d MA")
        ax.set_title("Daily revenue")
        ax.legend()
        st.pyplot(fig)

        # Monthly revenue
        st.markdown("### üìÖ Monthly Revenue")
        df["month"] = df["Date"].dt.to_period("M").astype(str)
        rev_mon = df.groupby("month")["price"].sum()
        fig, ax = plt.subplots(figsize=(8, 4))
        ax.plot(rev_mon.index, rev_mon.values, marker="o")
        ax.set_title("Monthly revenue")
        plt.xticks(rotation=45)
        st.pyplot(fig)

        # Deduplication
        dup_mask = df.duplicated(subset=["Member_number","Date","price"], keep="first")
        dup_n = int(dup_mask.sum()); dup_pct = dup_n / max(len(df), 1)
        st.markdown("### üîé Duplicate Check")
        st.write(f"- S·ªë d√≤ng tr√πng: {dup_n} / {len(df):,} ({dup_pct:.2%})")
        if dup_n > 0:
            st.write("- V√≠ d·ª• tr√πng l·∫∑p:")
            st.dataframe(df.loc[dup_mask].head(10))
        df_dedup = df.drop_duplicates(subset=["Member_number","Date","price"]).copy()
        st.write(f"- Sau drop_duplicates: {len(df_dedup):,} d√≤ng (gi·∫£m {dup_n:,})")

        # DAILY COLLAPSE

        st.markdown("## üóì Daily Collapse (customer-day level)")
        df_daily = df.groupby(['Member_number', 'Day']).agg(
            Sales=('Sales', 'sum')
        ).reset_index()

        st.write(f"Sau khi g·ªôp: {len(df):,} d√≤ng ‚Üí {len(df_daily):,} d√≤ng")
        st.dataframe(df_daily.head())

        # L∆∞u l·∫°i df_daily ƒë·ªÉ Modeling d√πng
        st.session_state['df_daily'] = df_daily

        # RFM Calculation

        st.markdown("## üßÆ RFM Calculation (sau khi g·ªôp)")
        max_date = df_daily['Day'].max()

        # Recency
        recency = df_daily.groupby('Member_number')["Day"].max().reset_index()
        recency['Recency'] = (max_date - recency['Day']).dt.days

        # Frequency
        frequency = df_daily.groupby('Member_number')["Day"].nunique().reset_index()
        frequency.columns = ['Member_number', 'Frequency']

        # Monetary
        monetary = df_daily.groupby('Member_number')["Sales"].sum().reset_index()
        monetary.columns = ['Member_number', 'Monetary']

        # Merge RFM
        rfm = recency[['Member_number', 'Recency']] \
                .merge(frequency, on='Member_number') \
                .merge(monetary, on='Member_number')

        st.write("### RFM Table (raw)")
        st.dataframe(rfm.head())

        # Winsorize + Scaling
        st.markdown("## ‚öñÔ∏è Winsorize + Scaling")
        rfm_scaled = rfm.copy()
        for col in ["Recency", "Frequency", "Monetary"]:
            rfm_scaled[col] = np.array(winsorize(rfm_scaled[col], limits=[0.01, 0.01]))
        
        robust = RobustScaler()
        rfm_scaled[["Recency", "Frequency", "Monetary"]] = robust.fit_transform(
            rfm_scaled[["Recency", "Frequency", "Monetary"]]
        )
        mm = MinMaxScaler()
        rfm_scaled[["Recency", "Frequency", "Monetary"]] = mm.fit_transform(
            rfm_scaled[["Recency", "Frequency", "Monetary"]]
        )

        st.write("### RFM Table (sau Winsorize + Scaling)")
        st.dataframe(rfm_scaled.head())

        # L∆∞u v√†o session_state
        st.session_state['rfm'] = rfm
        st.session_state['rfm_scaled'] = rfm_scaled

        st.success("‚úÖ RFM ƒë√£ ƒë∆∞·ª£c t√≠nh to√°n v√† scale th√†nh c√¥ng. S·∫µn s√†ng cho b∆∞·ªõc Modeling!")

    else:
        st.warning("‚ö†Ô∏è B·∫°n c·∫ßn chu·∫©n b·ªã d·ªØ li·ªáu ·ªü b∆∞·ªõc Data Understanding tr∆∞·ªõc.")


# Modeling & Evaluation     

elif choice == 'Modeling & Evaluation':
    st.write("### Modeling With KMeans")

    if 'df_daily' in st.session_state and st.session_state['df_daily'] is not None:
        df_daily = st.session_state['df_daily'].copy()

        # 1. T√≠nh RFM
        recent_date = df_daily['Day'].max()
        df_RFM = df_daily.groupby('Member_number').agg({
            'Day': lambda x: (recent_date - x.max()).days,
            'Member_number': 'count',
            'Sales': 'sum'
        }).rename(columns={
            'Day': 'Recency',
            'Member_number': 'Frequency',
            'Sales': 'Monetary'
        }).reset_index()

        st.subheader("üìä RFM Table (sau khi Daily Collapse)")
        st.dataframe(df_RFM.head())

        # 2. Ti·ªÅn x·ª≠ l√Ω (log, winsorize, scaling)
        X = df_RFM[['Recency', 'Frequency', 'Monetary']].copy()
        for col in X.columns:
            X[col] = np.log1p(X[col])
            X[col] = np.array(winsorize(X[col], limits=[0.01, 0.01]))

        robust = RobustScaler()
        X_scaled = robust.fit_transform(X)
        mm = MinMaxScaler()
        X_final = mm.fit_transform(X_scaled)

        # 3. Elbow method
        sse = {}
        for k in range(1, 15):
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            kmeans.fit(X_final)
            sse[k] = kmeans.inertia_

        fig, ax = plt.subplots()
        ax.set_title('Ph∆∞∆°ng ph√°p Elbow')
        ax.set_xlabel('S·ªë c·ª•m (k)')
        ax.set_ylabel('T·ªïng B√¨nh ph∆∞∆°ng kho·∫£ng c√°ch')
        sns.pointplot(x=list(sse.keys()), y=list(sse.values()), ax=ax)
        st.pyplot(fig)

        # 4. Ch·ªçn s·ªë cluster
        n_clusters = st.sidebar.number_input(
            'Ch·ªçn s·ªë l∆∞·ª£ng c·ª•m k (2-10):',
            min_value=2, max_value=10, value=4, step=1
        )
        model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        model.fit(X_final)

        df_RFM['Cluster'] = model.labels_

        # 5. Th·ªëng k√™ theo cluster
        cluster_stats = df_RFM.groupby('Cluster').agg({
            'Recency': 'mean',
            'Frequency': 'mean',
            'Monetary': ['mean', 'count']
        }).round(2)
        cluster_stats.columns = ['RecencyMean', 'FrequencyMean', 'MonetaryMean', 'Count']
        cluster_stats['Percent'] = (cluster_stats['Count'] / cluster_stats['Count'].sum() * 100).round(2)
        cluster_stats.reset_index(inplace=True)

        # 6. G√°n Segment
        def assign_segment(row, df_RFM):
            R_med, F_med, M_med = (
                df_RFM['Recency'].median(),
                df_RFM['Frequency'].median(),
                df_RFM['Monetary'].median()
            )
            R_q75 = df_RFM['Recency'].quantile(0.75)

            if row['RecencyMean'] < R_med and row['FrequencyMean'] > F_med and row['MonetaryMean'] > M_med:
                return "Champions"
            elif row['FrequencyMean'] > F_med and row['MonetaryMean'] > M_med:
                return "Loyal Customers"
            elif row['RecencyMean'] < R_med and row['FrequencyMean'] <= F_med:
                return "Potential Loyalist"
            elif row['MonetaryMean'] > M_med and row['FrequencyMean'] <= F_med:
                return "Big Spenders"
            elif row['RecencyMean'] > R_q75 and row['MonetaryMean'] > M_med:
                return "At Risk"
            elif row['RecencyMean'] > R_q75 and row['MonetaryMean'] <= M_med:
                return "Hibernating"
            else:
                return "Lost"

        cluster_stats['Segment'] = cluster_stats.apply(assign_segment, axis=1, df_RFM=df_RFM)

        st.subheader("üìà Th·ªëng k√™ theo c·ª•m + Segment")
        st.dataframe(cluster_stats)

        # 7. B·∫£ng gi·∫£i th√≠ch Segment
        st.subheader("üìñ Gi·∫£i th√≠ch Segment & Chi·∫øn l∆∞·ª£c kinh doanh")
        segment_explain = pd.DataFrame([
            ["Champions", "Kh√°ch m·ªõi mua g·∫ßn ƒë√¢y, mua nhi·ªÅu, chi ti√™u cao", "ChƒÉm s√≥c VIP, ∆∞u ƒë√£i ƒë·∫∑c bi·ªát, early access s·∫£n ph·∫©m"],
            ["Loyal Customers", "Kh√°ch trung th√†nh, mua th∆∞·ªùng xuy√™n, gi√° tr·ªã cao", "Duy tr√¨ loyalty program, upsell, x√¢y c·ªông ƒë·ªìng"],
            ["Potential Loyalist", "Kh√°ch m·ªõi, c√≥ ti·ªÅm nƒÉng tr·ªü th√†nh trung th√†nh", "T·∫∑ng voucher, remarketing khuy·∫øn kh√≠ch quay l·∫°i"],
            ["Big Spenders", "Mua √≠t nh∆∞ng chi ti√™u l·ªõn", "Upsell, cross-sell, g·ª£i √Ω sp cao c·∫•p"],
            ["At Risk", "T·ª´ng mua nhi·ªÅu nh∆∞ng ƒëang √≠t quay l·∫°i", "Re-engagement: email, khuy·∫øn m√£i quay l·∫°i"],
            ["Hibernating", "Kh√°ch c≈©, l√¢u r·ªìi kh√¥ng quay l·∫°i, chi ti√™u th·∫•p", "Remarketing nh·∫π nh√†ng, kh√¥ng ƒë·∫ßu t∆∞ qu√° nhi·ªÅu"],
            ["Lost", "Kh√°ch ƒë√£ r·ªùi b·ªè, √≠t gi√° tr·ªã", "Ch·ªâ ti·∫øp c·∫≠n khi c√≥ campaign l·ªõn"]
        ], columns=["Segment", "√ù nghƒ©a", "Chi·∫øn l∆∞·ª£c kinh doanh"])
        st.dataframe(segment_explain)

        # 8. Visualization
        fig_scatter = px.scatter(
            cluster_stats, x='RecencyMean', y='MonetaryMean',
            size='FrequencyMean', color='Segment', size_max=60
        )
        st.plotly_chart(fig_scatter, use_container_width=True)

        fig_3d = px.scatter_3d(
            cluster_stats, x='RecencyMean', y='FrequencyMean', z='MonetaryMean',
            color='Segment', size='Count'
        )
        st.plotly_chart(fig_3d, use_container_width=True)

        st.subheader("üó∫Ô∏è TreeMap ph√¢n kh√∫c kh√°ch h√†ng")
        fig_treemap, ax_treemap = plt.subplots()
        fig_treemap.set_size_inches(14, 10)
        squarify.plot(
            sizes=cluster_stats['Count'],
            label=[
                f"{row.Segment}\n"
                f"Recency: {row.RecencyMean:.1f}\n"
                f"Freq: {row.FrequencyMean:.1f}\n"
                f"Monetary: {row.MonetaryMean:.1f}\n"
                f"{row.Count} KH ({row.Percent}%)"
                for _, row in cluster_stats.iterrows()
            ],
            color=sns.color_palette("tab10", len(cluster_stats)).as_hex(),
            alpha=0.7,
            text_kwargs={'fontsize': 12, 'fontweight': 'bold'}
        )
        ax_treemap.set_title("Ph√¢n kh√∫c kh√°ch h√†ng (TreeMap)", fontsize=22, fontweight="bold")
        ax_treemap.axis('off')
        st.pyplot(fig_treemap)

        # 9. Export Model
        if st.button("üíæ Xu·∫•t m√¥ h√¨nh"):
            with open('kmeans_model.pkl', 'wb') as f:
                pickle.dump((model, cluster_stats, robust, mm), f)
            st.session_state['model_exported'] = True
            st.success("‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán v√† l∆∞u th√†nh c√¥ng! B√¢y gi·ªù b·∫°n c√≥ th·ªÉ sang tab Predict ƒë·ªÉ d·ª± ƒëo√°n.")

    else:
        st.warning("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu. Vui l√≤ng ch·∫°y b∆∞·ªõc Data Preparation tr∆∞·ªõc.")


# Predict

elif choice == 'Predict':
    st.subheader("D·ª± ƒëo√°n C·ª•m cho Kh√°ch h√†ng m·ªõi")

    if 'model_exported' in st.session_state and st.session_state['model_exported']:
        # Load l·∫°i m√¥ h√¨nh v√† cluster_stats
        with open('kmeans_model.pkl', 'rb') as f:
            model, cluster_stats, robust, mm = pickle.load(f)

        st.write("### üìä Th·ªëng k√™ c·ª•m hi·ªán t·∫°i")
        st.dataframe(cluster_stats)

        # 1. Upload file KH m·ªõi
        uploaded_predict_file = st.file_uploader("üìÇ Upload file kh√°ch h√†ng (CSV/Excel)", type=["csv", "xlsx"])
        if 'df_new' not in st.session_state:
            st.session_state['df_new'] = pd.DataFrame(columns=['Member_number', 'Day', 'Sales'])

        if uploaded_predict_file is not None:
            try:
                if uploaded_predict_file.name.endswith(".csv"):
                    df_uploaded = pd.read_csv(uploaded_predict_file)
                else:
                    df_uploaded = pd.read_excel(uploaded_predict_file)

                required_cols = ['Member_number', 'Day', 'Sales']
                if all(col in df_uploaded.columns for col in required_cols):
                    df_uploaded['Day'] = pd.to_datetime(df_uploaded['Day'])
                    st.session_state['df_new'] = pd.concat([st.session_state['df_new'], df_uploaded], ignore_index=True)
                    st.success("‚úÖ ƒê√£ th√™m d·ªØ li·ªáu t·ª´ file upload!")
                else:
                    st.error(f"‚ùå File ph·∫£i c√≥ ƒë·ªß c√°c c·ªôt: {required_cols}")
            except Exception as e:
                st.error(f"‚ö†Ô∏è L·ªói khi ƒë·ªçc file: {e}")

        # 2. Nh·∫≠p d·ªØ li·ªáu kh√°ch h√†ng m·ªõi
        member_number = st.text_input("üÜî M√£ Kh√°ch h√†ng:")
        last_purchase = st.date_input("üìÖ Ng√†y mua g·∫ßn nh·∫•t:")
        sales = st.number_input("üí∞ S·ªë ti·ªÅn giao d·ªãch:", min_value=0.0)

        if st.button("‚ûï Th√™m kh√°ch h√†ng"):
            new_data = pd.DataFrame({
                'Member_number': [member_number],
                'Day': [pd.to_datetime(last_purchase)],
                'Sales': [sales]
            })
            st.session_state['df_new'] = pd.concat([st.session_state['df_new'], new_data], ignore_index=True)

        st.write("### üì• D·ªØ li·ªáu kh√°ch h√†ng ƒë√£ th√™m")
        st.dataframe(st.session_state['df_new'])

        # 3. D·ª± ƒëo√°n
        if st.button("üöÄ D·ª± ƒëo√°n"):
            if len(st.session_state['df_new']) == 0:
                st.warning("‚ö†Ô∏è B·∫°n c·∫ßn nh·∫≠p √≠t nh·∫•t 1 kh√°ch h√†ng ƒë·ªÉ d·ª± ƒëo√°n.")
            else:
                df_new = st.session_state['df_new'].copy()

                # T√≠nh RFM cho d·ªØ li·ªáu m·ªõi
                max_date = pd.Timestamp.now().normalize()
                recency = df_new.groupby('Member_number')["Day"].max().reset_index()
                recency['Recency'] = (max_date - recency['Day']).dt.days

                frequency = df_new.groupby('Member_number')["Day"].nunique().reset_index()
                frequency.columns = ['Member_number', 'Frequency']

                monetary = df_new.groupby('Member_number')["Sales"].sum().reset_index()
                monetary.columns = ['Member_number', 'Monetary']

                df_RFM_new = recency[['Member_number', 'Recency']] \
                                .merge(frequency, on='Member_number') \
                                .merge(monetary, on='Member_number')

                st.write("### üßÆ B·∫£ng RFM cho KH m·ªõi")
                st.dataframe(df_RFM_new)

                # Winsorize + Scale
                X_new = df_RFM_new[['Recency', 'Frequency', 'Monetary']].copy()
                for col in X_new.columns:
                    X_new[col] = np.array(winsorize(X_new[col], limits=[0.01, 0.01]))
                X_new = robust.transform(X_new)
                X_new = mm.transform(X_new)

                # D·ª± ƒëo√°n Cluster
                cluster_pred = model.predict(X_new)
                df_RFM_new['Cluster'] = cluster_pred

                # G√°n Segment d·ª±a tr√™n cluster_stats
                cluster_map = cluster_stats.set_index("Cluster")["Segment"].to_dict()
                df_RFM_new['Segment'] = df_RFM_new['Cluster'].map(cluster_map)

                # G√°n chi·∫øn l∆∞·ª£c kinh doanh
                strategy_map = {
                    "Champions": "ChƒÉm s√≥c VIP, ∆∞u ƒë√£i ƒë·∫∑c bi·ªát, early access s·∫£n ph·∫©m",
                    "Loyal Customers": "Duy tr√¨ loyalty program, upsell, x√¢y c·ªông ƒë·ªìng",
                    "Potential Loyalist": "T·∫∑ng voucher, remarketing khuy·∫øn kh√≠ch quay l·∫°i",
                    "Big Spenders": "Upsell, cross-sell, g·ª£i √Ω sp cao c·∫•p",
                    "At Risk": "Re-engagement: email, khuy·∫øn m√£i quay l·∫°i",
                    "Hibernating": "Remarketing nh·∫π nh√†ng, kh√¥ng ƒë·∫ßu t∆∞ qu√° nhi·ªÅu",
                    "Lost": "Ch·ªâ ti·∫øp c·∫≠n khi c√≥ campaign l·ªõn"
                }
                df_RFM_new['Chi·∫øn l∆∞·ª£c'] = df_RFM_new['Segment'].map(strategy_map)

                # Hi·ªÉn th·ªã k·∫øt qu·∫£
                st.success("‚úÖ D·ª± ƒëo√°n th√†nh c√¥ng!")
                st.write("### üìå K·∫øt qu·∫£ d·ª± ƒëo√°n")
                st.dataframe(df_RFM_new)

                # Cho ph√©p t·∫£i k·∫øt qu·∫£
                csv_download_link(df_RFM_new, 'RFM_prediction_results.csv', 'üì• T·∫£i xu·ªëng k·∫øt qu·∫£')

    else:
        st.warning("‚ùå B·∫°n ph·∫£i xu·∫•t m√¥ h√¨nh tr∆∞·ªõc khi d·ª± ƒëo√°n.")

    # User Feedback

    st.write("### üí¨ User Feedback")
    user_feedback = st.text_area("Chia s·∫ª √Ω ki·∫øn c·ªßa b·∫°n:", value='')

    if st.button("üì© G·ª≠i Feedback"):
        current_time = datetime.now()
        feedback_df = pd.DataFrame({
            'Time': [current_time],
            'Feedback': [user_feedback]
        })

        if not os.path.isfile('feedback.csv'):
            feedback_df.to_csv('feedback.csv', index=False)
        else:
            feedback_df.to_csv('feedback.csv', mode='a', header=False, index=False)

        st.success("üëç C·∫£m ∆°n b·∫°n ƒë√£ g·ª≠i ph·∫£n h·ªìi!")





