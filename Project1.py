import os
import base64
from datetime import datetime

import pandas as pd
import numpy as np

import matplotlib
matplotlib.use("Agg")  
import matplotlib.pyplot as plt

import seaborn as sns
import plotly.express as px

from sklearn.cluster import KMeans
from sklearn.preprocessing import RobustScaler, MinMaxScaler

import pickle
import streamlit as st
import squarify

# GUI setup
st.set_page_config(layout="wide")
st.title("Data Science & Machine Learning Project")
st.header("Customer Segmentation", divider='rainbow')

# Menu 
menu = ["Business Understanding", "Data Understanding", "Data Preparation", "Modeling & Evaluation", "Predict"]
choice = st.sidebar.selectbox('Menu', menu)

def load_data(uploaded_file, default_path=None):
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file, encoding='latin-1')
        st.sidebar.success("File uploaded successfully!")
    elif default_path is not None and os.path.exists(default_path):
        df = pd.read_csv(default_path, encoding='latin-1')
        st.sidebar.info(f"D√πng file m·∫´u: {os.path.basename(default_path)}")
    else:
        df = None
    return df

def csv_download_link(df, csv_file_name, download_link_text):
    csv_data = df.to_csv(index=False)
    b64 = base64.b64encode(csv_data.encode()).decode()
    href = f'<a href="data:file/csv;base64,{b64}" download="{csv_file_name}">{download_link_text}</a>'
    st.markdown(href, unsafe_allow_html=True)

# Initializing session state variables
if 'df' not in st.session_state:
    st.session_state['df'] = None

# -------------------------
# Business Understanding
# -------------------------
if choice == 'Business Understanding':
    st.subheader("Business Objective")
    st.write("""
    ###### Ph√¢n kh√∫c/nh√≥m/c·ª•m kh√°ch h√†ng (market segmentation ‚Äì c√≤n ƒë∆∞·ª£c g·ªçi l√† ph√¢n kh√∫c th·ªã tr∆∞·ªùng) l√† qu√° tr√¨nh nh√≥m c√°c kh√°ch h√†ng l·∫°i v·ªõi nhau d·ª±a tr√™n c√°c ƒë·∫∑c ƒëi·ªÉm chung. N√≥ ph√¢n chia v√† nh√≥m kh√°ch h√†ng th√†nh c√°c nh√≥m nh·ªè theo ƒë·∫∑c ƒëi·ªÉm ƒë·ªãa l√Ω, nh√¢n kh·∫©u h·ªçc, t√¢m l√Ω h·ªçc, h√†nh vi (geographic, demographic, psychographic, behavioral) v√† c√°c ƒë·∫∑c ƒëi·ªÉm kh√°c.

    ###### C√°c nh√† ti·∫øp th·ªã s·ª≠ d·ª•ng k·ªπ thu·∫≠t n√†y ƒë·ªÉ:
    - **C√° nh√¢n h√≥a**: ƒêi·ªÅu ch·ªânh chi·∫øn l∆∞·ª£c ti·∫øp th·ªã ƒë·ªÉ ƒë√°p ·ª©ng nhu c·∫ßu ri√™ng c·ªßa t·ª´ng ph√¢n kh√∫c.
    - **T·ªëi ∆∞u h√≥a**: Ph√¢n b·ªï hi·ªáu qu·∫£ ngu·ªìn l·ª±c ti·∫øp th·ªã.
    - **Insight**: Hi·ªÉu s√¢u h∆°n v·ªÅ c∆° s·ªü kh√°ch h√†ng.
    - **T∆∞∆°ng t√°c**: N√¢ng cao s·ª± t∆∞∆°ng t√°c v√† s·ª± h√†i l√≤ng c·ªßa kh√°ch h√†ng.

    ###### => V·∫•n ƒë·ªÅ/Y√™u c·∫ßu: S·ª≠ d·ª•ng Machine Learning v√† ph√¢n t√≠ch d·ªØ li·ªáu trong Python ƒë·ªÉ th·ª±c hi·ªán ph√¢n kh√∫c kh√°ch h√†ng.
    """)
    st.image("RFM.png", caption="Customer Segmentation s·ª≠ d·ª•ng RFM", use_container_width=True)

# -------------------------
# Data Understanding
# -------------------------
elif choice == 'Data Understanding':
    st.subheader("Upload ho·∫∑c ch·ªçn d·ªØ li·ªáu")

    st.markdown("**üì¶ Product File (th√¥ng tin s·∫£n ph·∫©m)**")
    product_file = st.sidebar.file_uploader("Upload product file", type=['csv'], key="product")
    df_products = load_data(product_file, default_path="Data Project 1/Products_with_Categories.csv")

    st.markdown("**üõí Transaction File (giao d·ªãch)**")
    transaction_file = st.sidebar.file_uploader("Upload transaction file", type=['csv'], key="transaction")
    df_transactions = load_data(transaction_file, default_path="Data Project 1/Transactions.csv")

    if df_products is not None and df_transactions is not None:
        # n·∫øu c·ªôt t√™n kh√°c, b·∫°n c√≥ th·ªÉ s·ª≠a mapping tr∆∞·ªõc khi merge
        # ƒë·∫£m b·∫£o productId t·ªìn t·∫°i ·ªü c·∫£ 2
        if "productId" not in df_transactions.columns:
            st.error("Kh√¥ng t√¨m th·∫•y c·ªôt 'productId' trong transaction file.")
        else:
            df = pd.merge(df_transactions, df_products, on="productId", how="left")
            st.session_state['df'] = df

            st.write("### Data Overview")
            st.write("Number of rows:", df.shape[0])
            st.write("Number of columns:", df.shape[1])
            st.write("First five rows of the merged data:")
            st.write(df.head())

            csv_download_link(df, "merged_data.csv", "üì• Download merged CSV")
    else:
        st.warning("‚ö†Ô∏è Vui l√≤ng upload ho·∫∑c ki·ªÉm tra file m·∫´u trong th∆∞ m·ª•c `Data Project 1`")

from scipy.stats.mstats import winsorize

# Data Preparation
# -------------------------
if choice == 'Data Preparation':
    st.subheader("Data Preparation & EDA")

    if st.session_state['df'] is not None:
        df = st.session_state['df'].copy()

        # ƒê·∫£m b·∫£o Date v·ªÅ d·∫°ng datetime
        df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')
        df['Day'] = df['Date'].dt.to_period("D").dt.to_timestamp()

        # T·∫°o bi·∫øn Sales
        df['Sales'] = df['items'] * df['price']

        st.markdown("### üìä Data Summary")
        st.write(df.describe())

        # Missing values
        st.markdown("### üï≥ Missing Values")
        st.write(df.isna().sum())

        # Duplicate check
        dup_cnt = df.duplicated(subset=["Member_number", "Date", "price"]).sum()
        st.write(f"**Likely duplicates**: {dup_cnt} rows (subset on Member_number, Date, price)")

        # Histograms
        st.markdown("### Distribution of Price & log(1+Price)")
        fig, ax = plt.subplots(1, 2, figsize=(12, 4))
        ax[0].hist(df["price"], bins=40, alpha=.8)
        ax[0].set_title("Histogram price")
        ax[1].hist(np.log1p(df["price"]), bins=40, alpha=.8)
        ax[1].set_title("Histogram log(1+price)")
        st.pyplot(fig)

        # Daily revenue
        st.markdown("### üìà Daily Revenue with MA7")
        daily = df.groupby("Day")["price"].sum().sort_index()
        fig, ax = plt.subplots(figsize=(10, 4))
        ax.plot(daily.index, daily.values, alpha=.4, label="daily")
        ax.plot(daily.index, daily.rolling(7, min_periods=1).mean(), label="7d MA")
        ax.set_title("Daily revenue")
        ax.legend()
        st.pyplot(fig)

        # Monthly revenue
        st.markdown("### üìÖ Monthly Revenue")
        df["month"] = df["Date"].dt.to_period("M").astype(str)
        rev_mon = df.groupby("month")["price"].sum()
        fig, ax = plt.subplots(figsize=(8, 4))
        ax.plot(rev_mon.index, rev_mon.values, marker="o")
        ax.set_title("Monthly revenue")
        plt.xticks(rotation=45)
        st.pyplot(fig)

        # Deduplication
        dup_mask = df.duplicated(subset=["Member_number","Date","price"], keep="first")
        dup_n = int(dup_mask.sum()); dup_pct = dup_n / max(len(df), 1)
        st.markdown("### üîé Duplicate Check")
        st.write(f"- S·ªë d√≤ng tr√πng: {dup_n} / {len(df):,} ({dup_pct:.2%})")
        if dup_n > 0:
            st.write("- V√≠ d·ª• tr√πng l·∫∑p:")
            st.dataframe(df.loc[dup_mask].head(10))
        df_dedup = df.drop_duplicates(subset=["Member_number","Date","price"]).copy()
        st.write(f"- Sau drop_duplicates: {len(df_dedup):,} d√≤ng (gi·∫£m {dup_n:,})")

        # =====================
        # DAILY COLLAPSE
        # =====================
        st.markdown("## üóì Daily Collapse (customer-day level)")
        df_daily = df.groupby(['Member_number', 'Day']).agg(
            Sales=('Sales', 'sum')
        ).reset_index()

        st.write(f"Sau khi g·ªôp: {len(df):,} d√≤ng ‚Üí {len(df_daily):,} d√≤ng")
        st.dataframe(df_daily.head())

        # L∆∞u l·∫°i df_daily ƒë·ªÉ Modeling d√πng
        st.session_state['df_daily'] = df_daily

        # =====================
        # RFM Calculation
        # =====================
        st.markdown("## üßÆ RFM Calculation (sau khi g·ªôp)")
        max_date = df_daily['Day'].max()

        # Recency
        recency = df_daily.groupby('Member_number')["Day"].max().reset_index()
        recency['Recency'] = (max_date - recency['Day']).dt.days

        # Frequency
        frequency = df_daily.groupby('Member_number')["Day"].nunique().reset_index()
        frequency.columns = ['Member_number', 'Frequency']

        # Monetary
        monetary = df_daily.groupby('Member_number')["Sales"].sum().reset_index()
        monetary.columns = ['Member_number', 'Monetary']

        # Merge RFM
        rfm = recency[['Member_number', 'Recency']] \
                .merge(frequency, on='Member_number') \
                .merge(monetary, on='Member_number')

        st.write("### RFM Table (raw)")
        st.dataframe(rfm.head())

        # Winsorize + Scaling
        st.markdown("## ‚öñÔ∏è Winsorize + Scaling")
        rfm_scaled = rfm.copy()
        for col in ["Recency", "Frequency", "Monetary"]:
            rfm_scaled[col] = winsorize(rfm_scaled[col], limits=[0.01, 0.01])

        robust = RobustScaler()
        rfm_scaled[["Recency", "Frequency", "Monetary"]] = robust.fit_transform(
            rfm_scaled[["Recency", "Frequency", "Monetary"]]
        )
        mm = MinMaxScaler()
        rfm_scaled[["Recency", "Frequency", "Monetary"]] = mm.fit_transform(
            rfm_scaled[["Recency", "Frequency", "Monetary"]]
        )

        st.write("### RFM Table (sau Winsorize + Scaling)")
        st.dataframe(rfm_scaled.head())

        # L∆∞u v√†o session_state
        st.session_state['rfm'] = rfm
        st.session_state['rfm_scaled'] = rfm_scaled

        st.success("‚úÖ RFM ƒë√£ ƒë∆∞·ª£c t√≠nh to√°n v√† scale th√†nh c√¥ng. S·∫µn s√†ng cho b∆∞·ªõc Modeling!")

    else:
        st.warning("‚ö†Ô∏è B·∫°n c·∫ßn chu·∫©n b·ªã d·ªØ li·ªáu ·ªü b∆∞·ªõc Data Understanding tr∆∞·ªõc.")


# Modeling & Evaluation
# -------------------------
elif choice == 'Modeling & Evaluation':
    st.write("### Modeling With KMeans")

    if 'df_daily' in st.session_state and st.session_state['df_daily'] is not None:
        df_daily = st.session_state['df_daily'].copy()

        # =========================
        # 1. T√≠nh RFM t·ª´ df_daily
        # =========================
        recent_date = df_daily['Day'].max()
        df_RFM = df_daily.groupby('Member_number').agg({
            'Day': lambda x: (recent_date - x.max()).days,
            'Member_number': 'count',
            'Sales': 'sum'
        }).rename(columns={
            'Day': 'Recency',
            'Member_number': 'Frequency',
            'Sales': 'Monetary'
        }).reset_index()

        st.subheader("üìä RFM Table (sau khi Daily Collapse)")
        st.dataframe(df_RFM.head())

        # =========================
        # 2. WINSORIZE + SCALING
        # =========================
        X = df_RFM[['Recency', 'Frequency', 'Monetary']].copy()

        # Winsorize tr∆∞·ªõc
        for col in X.columns:
            X[col] = winsorize(X[col], limits=[0.01, 0.01])

        # RobustScaler
        robust = RobustScaler()
        X_scaled = robust.fit_transform(X)

        # MinMaxScaler
        mm = MinMaxScaler()
        X_final = mm.fit_transform(X_scaled)

        # =========================
        # 3. ELBOW METHOD
        # =========================
        sse = {}
        for k in range(1, 15):
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            kmeans.fit(X_final)
            sse[k] = kmeans.inertia_

        fig, ax = plt.subplots()
        ax.set_title('Ph∆∞∆°ng ph√°p Elbow')
        ax.set_xlabel('S·ªë c·ª•m (k)')
        ax.set_ylabel('T·ªïng B√¨nh ph∆∞∆°ng kho·∫£ng c√°ch')
        sns.pointplot(x=list(sse.keys()), y=list(sse.values()), ax=ax)
        st.pyplot(fig)

        # =========================
        # 4. FIT KMEANS
        # =========================
        n_clusters = st.sidebar.number_input(
            'Ch·ªçn s·ªë l∆∞·ª£ng c·ª•m k (2-10):',
            min_value=2, max_value=10, value=3, step=1
        )
        model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        model.fit(X_final)

        df_RFM['Cluster'] = model.labels_

        # =========================
        # 5. CLUSTER STATS
        # =========================
        cluster_stats = df_RFM.groupby('Cluster').agg({
            'Recency': 'mean',
            'Frequency': 'mean',
            'Monetary': ['mean', 'count']
        }).round(2)
        cluster_stats.columns = ['RecencyMean', 'FrequencyMean', 'MonetaryMean', 'Count']
        cluster_stats['Percent'] = (cluster_stats['Count'] / cluster_stats['Count'].sum() * 100).round(2)
        cluster_stats.reset_index(inplace=True)
        cluster_stats['Cluster'] = 'C·ª•m ' + cluster_stats['Cluster'].astype(str)

        st.subheader("üìà Th·ªëng k√™ theo c·ª•m")
        st.dataframe(cluster_stats)

        # =========================
        # 6. N√öT EXPORT MODEL
        # =========================
        if st.button("üíæ Xu·∫•t m√¥ h√¨nh"):
            with open('kmeans_model.pkl', 'wb') as f:
                pickle.dump((model, cluster_stats, robust, mm), f)
            st.session_state['model_exported'] = True
            st.success("‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán v√† l∆∞u th√†nh c√¥ng! B√¢y gi·ªù b·∫°n c√≥ th·ªÉ sang tab Predict ƒë·ªÉ d·ª± ƒëo√°n.")

        # =========================
        # 7. VISUALIZATION
        # =========================
        fig_scatter = px.scatter(
            cluster_stats, x='RecencyMean', y='MonetaryMean',
            size='FrequencyMean', color='Cluster', size_max=60
        )
        st.plotly_chart(fig_scatter, use_container_width=True)

        fig_3d = px.scatter_3d(
            cluster_stats, x='RecencyMean', y='FrequencyMean', z='MonetaryMean',
            color='Cluster', size='Count'
        )
        st.plotly_chart(fig_3d, use_container_width=True)

        st.subheader("üó∫Ô∏è TreeMap ph√¢n kh√∫c kh√°ch h√†ng")
        colors_dict = {
            0: 'green', 1: 'red', 2: 'royalblue',
            3: 'orange', 4: 'purple', 5: 'pink',
            6: 'brown', 7: 'cyan', 8: 'yellow', 9: 'gray'
        }

        fig_treemap, ax_treemap = plt.subplots()
        fig_treemap.set_size_inches(14, 10)
        squarify.plot(
            sizes=cluster_stats['Count'],
            label=[
                f"{row.Cluster}\n"
                f"Recency: {row.RecencyMean:.1f}\n"
                f"Freq: {row.FrequencyMean:.1f}\n"
                f"Monetary: {row.MonetaryMean:.1f}\n"
                f"{row.Count} KH ({row.Percent}%)"
                for _, row in cluster_stats.iterrows()
            ],
            color=[colors_dict.get(i, 'gray') for i in range(len(cluster_stats))],
            alpha=0.7,
            text_kwargs={'fontsize': 12, 'fontweight': 'bold'}
        )
        ax_treemap.set_title("Ph√¢n kh√∫c kh√°ch h√†ng (TreeMap)", fontsize=22, fontweight="bold")
        ax_treemap.axis('off')
        st.pyplot(fig_treemap)

    else:
        st.warning("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu. Vui l√≤ng ch·∫°y b∆∞·ªõc Data Preparation tr∆∞·ªõc.")

# Predict
# -------------------------
elif choice == 'Predict':
    st.subheader("üîÆ D·ª± ƒëo√°n C·ª•m cho Kh√°ch h√†ng m·ªõi")

    if 'model_exported' in st.session_state and st.session_state['model_exported']:
        # Load l·∫°i m√¥ h√¨nh v√† cluster_stats
        with open('kmeans_model.pkl', 'rb') as f:
            model, cluster_stats, robust, mm = pickle.load(f)

        st.write("### üìä Th·ªëng k√™ c·ª•m hi·ªán t·∫°i")
        st.dataframe(cluster_stats)

        # Nh·∫≠p d·ªØ li·ªáu kh√°ch h√†ng m·ªõi
        member_number = st.text_input("üÜî M√£ Kh√°ch h√†ng:")
        last_purchase = st.date_input("üìÖ Ng√†y mua g·∫ßn nh·∫•t:")
        sales = st.number_input("üí∞ S·ªë ti·ªÅn giao d·ªãch:", min_value=0.0)

        # L∆∞u d·ªØ li·ªáu nh·∫≠p v√†o session_state
        if 'df_new' not in st.session_state:
            st.session_state['df_new'] = pd.DataFrame(columns=['Member_number', 'Day', 'Sales'])

        if st.button("‚ûï Th√™m kh√°ch h√†ng"):
            new_data = pd.DataFrame({
                'Member_number': [member_number],
                'Day': [pd.to_datetime(last_purchase)],
                'Sales': [sales]
            })
            st.session_state['df_new'] = pd.concat([st.session_state['df_new'], new_data], ignore_index=True)

        st.write("### üì• D·ªØ li·ªáu kh√°ch h√†ng ƒë√£ th√™m")
        st.dataframe(st.session_state['df_new'])

        # N√∫t d·ª± ƒëo√°n
        if st.button("üöÄ D·ª± ƒëo√°n"):
            if len(st.session_state['df_new']) == 0:
                st.warning("‚ö†Ô∏è B·∫°n c·∫ßn nh·∫≠p √≠t nh·∫•t 1 kh√°ch h√†ng ƒë·ªÉ d·ª± ƒëo√°n.")
            else:
                df_new = st.session_state['df_new'].copy()

                # =========================
                # 1. T√≠nh RFM cho d·ªØ li·ªáu m·ªõi
                # =========================
                max_date = pd.Timestamp.now().normalize()
                recency = df_new.groupby('Member_number')["Day"].max().reset_index()
                recency['Recency'] = (max_date - recency['Day']).dt.days

                frequency = df_new.groupby('Member_number')["Day"].nunique().reset_index()
                frequency.columns = ['Member_number', 'Frequency']

                monetary = df_new.groupby('Member_number')["Sales"].sum().reset_index()
                monetary.columns = ['Member_number', 'Monetary']

                df_RFM_new = recency[['Member_number', 'Recency']] \
                                .merge(frequency, on='Member_number') \
                                .merge(monetary, on='Member_number')

                st.write("### üßÆ B·∫£ng RFM cho KH m·ªõi")
                st.dataframe(df_RFM_new)

                # =========================
                # 2. Winsorize + Scale d·ªØ li·ªáu m·ªõi
                # =========================
                X_new = df_RFM_new[['Recency', 'Frequency', 'Monetary']].copy()

                # Winsorize nh∆∞ l√∫c train
                for col in X_new.columns:
                    X_new[col] = winsorize(X_new[col], limits=[0.01, 0.01])

                # Scale b·∫±ng RobustScaler + MinMaxScaler ƒë√£ fit
                X_new = robust.transform(X_new)
                X_new = mm.transform(X_new)

                # =========================
                # 3. D·ª± ƒëo√°n
                # =========================
                cluster_pred = model.predict(X_new)
                df_RFM_new['Cluster'] = cluster_pred

                st.success("‚úÖ D·ª± ƒëo√°n th√†nh c√¥ng!")
                st.write("### üìå K·∫øt qu·∫£ d·ª± ƒëo√°n")
                st.dataframe(df_RFM_new)

                # Cho ph√©p t·∫£i k·∫øt qu·∫£
                csv_download_link(df_RFM_new, 'RFM_prediction_results.csv', 'üì• T·∫£i xu·ªëng k·∫øt qu·∫£')

    else:
        st.warning("‚ùå B·∫°n ph·∫£i xu·∫•t m√¥ h√¨nh tr∆∞·ªõc khi d·ª± ƒëo√°n.")

    # =====================
    # User Feedback
    # =====================
    st.write("### üí¨ User Feedback")
    user_feedback = st.text_area("Chia s·∫ª √Ω ki·∫øn c·ªßa b·∫°n:", value='')

    if st.button("üì© G·ª≠i Feedback"):
        current_time = datetime.now()
        feedback_df = pd.DataFrame({
            'Time': [current_time],
            'Feedback': [user_feedback]
        })

        if not os.path.isfile('feedback.csv'):
            feedback_df.to_csv('feedback.csv', index=False)
        else:
            feedback_df.to_csv('feedback.csv', mode='a', header=False, index=False)

        st.success("üëç C·∫£m ∆°n b·∫°n ƒë√£ g·ª≠i ph·∫£n h·ªìi!")





